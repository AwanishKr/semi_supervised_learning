{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport copy\nimport random\nimport pickle\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom scipy.stats import entropy\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset,DataLoader, Subset\nimport PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\nimport torchvision.transforms.functional as F\nfrom efficientnet_pytorch import EfficientNet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\ntrain_transforms = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=0.25),\n        transforms.RandomVerticalFlip(p=0.25),\n        transforms.RandomAffine(degrees=20, translate=(0.25, 0.25), shear=(-0.25, 0.25)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n\ntest_transforms = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unpickle(file):\n    with open(file, 'rb') as fo:\n        dictt = pickle.load(fo, encoding='latin1')\n    return dictt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read dictionary\ndata_train = unpickle(\"../input/cifar100/train\")\ndata_test = unpickle(\"../input/cifar100/test\")\ndata_meta = unpickle(\"../input/cifar100/meta\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subCategory = pd.DataFrame(data_meta['fine_label_names'], columns=['SubClass'])\nsubCategoryDict = subCategory.to_dict()\nfor key in list(subCategoryDict.keys()):\n    print(subCategoryDict[key])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only use 10000 labelled samples\n\nx_train, y_train = data_train['data'][0:10000], data_train['fine_labels'][0:10000]\nx_test, y_test = data_test['data'], data_test['fine_labels']\nprint(x_train.shape, x_test.shape)\nprint(type(x_train), type(y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = x_train.reshape(len(x_train),3,32,32)\nx_test = x_test.reshape(len(x_test),3,32,32)\n\nx_train, y_train = np.asarray(x_train), np.asarray(y_train)\nx_test, y_test = np.asarray(x_test), np.asarray(y_test)\n\nprint(type(x_train), type(y_train), type(x_test), type(y_test))\nprint(y_train.shape, y_test.shape)\nprint(np.max(y_train), np.max(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, data, targets, transform=None):\n        self.data = data\n        self.targets = torch.LongTensor(targets)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.targets[index]\n        if self.transform:\n            # converting the array into a PIL image\n            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n            x = self.transform(x)\n        return {'images': x, 'labels': y}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = MyDataset(x_train, y_train, transform=train_transforms)\ntest_dataset = MyDataset(x_test, y_test, transform=test_transforms)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n\ndataloaders = {'train': train_dataloader, 'test':test_dataloader}\ndataset_sizes = {'train': len(train_dataset), 'test':len(test_dataset)}\n\nprint(dataset_sizes['train'], dataset_sizes['test'])\nprint(len(train_dataloader), len(test_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_unlabelled = data_train['data'][10000:30000]\nx_unlabelled = x_unlabelled.reshape(len(x_unlabelled),3,32,32)\nx_unlabelled = np.asarray(x_unlabelled)\nprint(x_unlabelled.shape, type(x_unlabelled))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UnlabelledDataset(Dataset):\n    def __init__(self, data, m, transforms=None):\n        self.data = data\n        self.transforms = transforms\n        self.images_list = list(np.arange(1, (len(self.data)+1)))\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        image = self.data[index]\n        image = Image.fromarray(image.astype(np.uint8).transpose(1,2,0))\n        if self.transforms is not None:\n            image = self.transforms(image)\n        return {'image_names': index, 'images': image}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unlabelled_dataset = UnlabelledDataset(x_unlabelled, m=4, transforms=test_transforms)\nunlabelled_dataloader = DataLoader(unlabelled_dataset, batch_size=256, num_workers=8, shuffle=False, pin_memory=True)\n\nprint(len(unlabelled_dataset))\nprint(len(unlabelled_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Baseline_Efficient(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base_model = EfficientNet.from_name('efficientnet-b0')\n        self.linear = nn.Linear(in_features=self.base_model._fc.in_features, out_features=100)\n        self.base_model._fc = self.linear\n\n    def forward(self, x):\n        x = self.base_model(x)\n        return x\n\n\nclass Baseline_WideResNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base_model = torchvision.models.wide_resnet50_2(pretrained=False)\n        self.linear = nn.Linear(in_features=self.base_model.fc.in_features, out_features=100)\n        self.base_model.fc = self.linear\n\n    def forward(self, x):\n        x = self.base_model(x)\n        return x\n\n\nclass Baseline_ResNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base_model = torchvision.models.resnet18(pretrained=False)\n        self.linear = nn.Linear(in_features=512, out_features=100)\n        self.base_model.fc = self.linear\n\n    def forward(self, x):\n        x = self.base_model(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nres_path = '../input/resnet-model-cifar100-8000-samples-with-scheduler/resnet_cifar100_8000_samples_scheduler.pth'\neff_path = '../input/eff-net-model-cifar100-8000-samples/efficientNet_cifar100_8000_samples_scheduler.pth'\nwide_path = '../input/wide-net-model-cifar100-8000-samples/wide_resnet_cifar100_8000_samples_scheduler.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Baseline_model_res = Baseline_ResNet()\n# Baseline_model_eff = Baseline_Efficient()\n# Baseline_model_wide = Baseline_WideResNet()\n\n\n# Baseline_model_res.load_state_dict(torch.load('../input/resnet-model-cifar100-8000-samples-with-scheduler/resnet_cifar100_8000_samples_scheduler.pth'))\n# Baseline_model_eff.load_state_dict(torch.load('../input/eff-net-model-cifar100-8000-samples/efficientNet_cifar100_8000_samples_scheduler.pth'))\n# Baseline_model_wide.load_state_dict(torch.load('../input/wide-net-model-cifar100-8000-samples/wide_resnet_cifar100_8000_samples_scheduler.pth'))\n\n# Baseline_model_eff = Baseline_model_eff.to(device)\n# Baseline_model_res = Baseline_model_res.to(device)\n# Baseline_model_wide = Baseline_model_wide.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sharpen(vector, t=0.3):\n    n = np.power(vector, 1 / t)\n    d = np.sum(np.power(vector, 1 / t))\n    return n / d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# softmax = nn.Softmax(dim=1)\n# predictions = []\n# image_names = []\n\n# with torch.no_grad():\n#     Baseline_model_res = Baseline_model_res.eval()\n#     for sample in tqdm(unlabelled_dataloader):\n#         images = sample['images'].to(device)\n#         batch_size = images.size(0)\n#         num_ensemble = images.size(1)\n#         images = images.view(images.size(0) * images.size(1), images.size(2), images.size(3), images.size(4))\n#         output = softmax(Baseline_model_res(images))\n#         output = output.view(batch_size, num_ensemble, output.size(1))\n#         output = torch.mean(output, 1)\n#         predictions.append(output.cpu().numpy())\n#         image_names.append(sample['image_names'])\n\n# predictions_a_np = predictions[0]\n# for i in range(1, len(predictions)):\n#     predictions_a_np = np.concatenate((predictions_a_np, predictions[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = []\n# with torch.no_grad():\n#     Baseline_model_eff = Baseline_model_eff.eval()\n#     for sample in tqdm(unlabelled_dataloader):\n#         images = sample['images'].to(device)\n#         batch_size = images.size(0)\n#         num_ensemble = images.size(1)\n#         images = images.view(images.size(0) * images.size(1), images.size(2), images.size(3), images.size(4))\n#         output = softmax(Baseline_model_eff(images))\n#         output = output.view(batch_size, num_ensemble, output.size(1))\n#         output = torch.mean(output, 1)\n#         predictions.append(output.cpu().numpy())\n\n# predictions_b_np = predictions[0]\n# for i in range(1, len(predictions)):\n#     predictions_b_np = np.concatenate((predictions_b_np, predictions[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = []\n# with torch.no_grad():\n#     Baseline_model_wide = Baseline_model_wide.eval()\n#     for sample in tqdm(unlabelled_dataloader):\n#         images = sample['images'].to(device)\n#         batch_size = images.size(0)\n#         num_ensemble = images.size(1)\n#         images = images.view(images.size(0) * images.size(1), images.size(2), images.size(3), images.size(4))\n#         output = softmax(Baseline_model_wide(images))\n#         output = output.view(batch_size, num_ensemble, output.size(1))\n#         output = torch.mean(output, 1)\n#         predictions.append(output.cpu().numpy())\n\n# predictions_c_np = predictions[0]\n# for i in range(1, len(predictions)):\n#     predictions_c_np = np.concatenate((predictions_c_np, predictions[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pseudo_label(model_res_path, model_eff_path, model_wide_path, unlabelled_dataloader, device):\n    Baseline_model_res = Baseline_ResNet()\n    Baseline_model_eff = Baseline_Efficient()\n    Baseline_model_wide = Baseline_WideResNet()\n    \n    Baseline_model_res.load_state_dict(torch.load(model_res_path))\n    Baseline_model_eff.load_state_dict(torch.load(model_eff_path))\n    Baseline_model_wide.load_state_dict(torch.load(model_wide_path))\n    \n    Baseline_model_res = Baseline_model_res.to(device)\n    Baseline_model_eff = Baseline_model_eff.to(device)\n    Baseline_model_wide = Baseline_model_wide.to(device)\n    \n    softmax = nn.Softmax(dim=1)\n    predictions = []\n    image_names = []\n    with torch.no_grad():\n        Baseline_model_res = Baseline_model_res.eval()\n        for sample in tqdm(unlabelled_dataloader):\n            images = sample['images'].to(device)\n            output = softmax(Baseline_model_res(images))\n            predictions.append(output.cpu().numpy())\n            image_names.append(sample['image_names'])\n            \n    predictions_a_np = predictions[0]\n    for i in range(1, len(predictions)):\n        predictions_a_np = np.concatenate((predictions_a_np, predictions[i]))\n        \n        \n    predictions = []\n    with torch.no_grad():\n        Baseline_model_eff = Baseline_model_eff.eval()\n        for sample in tqdm(unlabelled_dataloader):\n            images = sample['images'].to(device)\n            output = softmax(Baseline_model_eff(images))\n            predictions.append(output.cpu().numpy())\n            \n    predictions_b_np = predictions[0]\n    for i in range(1, len(predictions)):\n        predictions_b_np = np.concatenate((predictions_b_np, predictions[i]))\n        \n        \n    predictions = []\n    with torch.no_grad():\n        Baseline_model_wide = Baseline_model_wide.eval()\n        for sample in tqdm(unlabelled_dataloader):\n            images = sample['images'].to(device)\n            output = softmax(Baseline_model_wide(images))\n            predictions.append(output.cpu().numpy())\n            \n    predictions_c_np = predictions[0]\n    for i in range(1, len(predictions)):\n        predictions_c_np = np.concatenate((predictions_c_np, predictions[i]))\n        \n        \n    ensemble_arr = np.mean((predictions_a_np, predictions_b_np, predictions_c_np), 0)\n    sharpened_arr = np.apply_along_axis(sharpen, 1, ensemble_arr)\n    image_names = list(itertools.chain.from_iterable(image_names))\n    \n    return sharpened_arr, image_names","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble_arr = np.mean((predictions_a_np, predictions_b_np, predictions_c_np), 0)\n# sharpened_arr = np.apply_along_axis(sharpen, 1, ensemble_arr)\n# image_names = list(itertools.chain.from_iterable(image_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def entropy_fn(vector):\n    return entropy(vector)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PseudoLabelledDataset(Dataset):\n    def __init__(self, data, predictions, image_names, topn, transforms=None):\n        super(PseudoLabelledDataset, self).__init__()\n        self.topn = topn\n        self.data = data\n        self.transforms = transforms\n        self.predictions = predictions\n        self.image_names = image_names\n        self.sorted_dataframe = self.sort_by_entropy()\n        self.sorted_dataframe = self.sorted_dataframe[:topn]\n    def sort_by_entropy(self):\n        sorted_dataframe = pd.DataFrame()\n        sorted_dataframe['image_names'] = self.image_names\n        sorted_dataframe['predictions'] = self.predictions.tolist()\n        sorted_dataframe['entropy'] = np.apply_along_axis(entropy_fn, 1, self.predictions)\n        sorted_dataframe = sorted_dataframe.sort_values(by='entropy', ascending=True).reset_index(drop=True)\n        return sorted_dataframe\n    \n    def __len__(self):\n        return self.topn\n\n    def __getitem__(self, idx):\n        image_name = self.sorted_dataframe.iloc[idx, 0]\n        image_prediction = self.sorted_dataframe.iloc[idx, 1]\n        image = self.data[idx]\n        image = Image.fromarray(image.astype(np.uint8).transpose(1,2,0))\n        if self.transforms:\n            image = self.transforms(image)\n        return {'images': image, 'labels': np.array(image_prediction, dtype=np.float32)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sharpened_arr, image_names = pseudo_label(res_path, eff_path, wide_path, unlabelled_dataloader, device)\npseudo_labelled_dataset = PseudoLabelledDataset(x_unlabelled, predictions=sharpened_arr,\n                                                image_names=image_names, topn=20000, transforms=train_transforms)\n\nN = 16000\nrandom_idx = random.sample(range(0, 20000), N)\nsampled_pseudo_dataset = Subset(pseudo_labelled_dataset, random_idx)\nsampled_pseudo_dataloader = DataLoader(sampled_pseudo_dataset, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\nprint('sampled_pseudo_dataset Size', len(sampled_pseudo_dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(y_true, y_pred):\n    return torch.sum(y_true == y_pred) / y_true.size(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.count = 0\n        self.sum = 0\n        self.avg = 0\n    def update(self, val, n):\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion, device, epoch, dataloader_size):\n    model = model.eval()\n    running_loss = AverageMeter()\n    running_accuracy = AverageMeter()\n    with torch.no_grad():\n        for i, sample in enumerate(dataloader):\n            inputs, labels = sample['images'].to(device), sample['labels'].to(device)\n            outputs = model(inputs)\n            _, predictions = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n            running_loss.update(loss.item(), inputs.size(0))\n            running_accuracy.update(accuracy(y_true=labels, y_pred=predictions), inputs.size(0))\n    return running_loss.avg, running_accuracy.avg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base_model = torchvision.models.resnet18(pretrained=True)\n        self.linear = nn.Linear(in_features=512, out_features=100)\n        self.base_model.fc = self.linear\n\n    def forward(self, x):\n        x = self.base_model(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_res = ResNet()\nmodel_res.to(device)\nlr = 0.0001\nweight_decay = 0.00001\noptimizer = optim.Adam(model_res.parameters(), lr = lr, weight_decay = weight_decay)\n\ncriterion1 = nn.CrossEntropyLoss()\ncriterion2 = nn.MSELoss()\nlr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[20, 30, 40], gamma=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, labelled_dataloader, pseudo_labelled_dataloader, optimizer, criterion1, criterion2, device, epoch, lambda_u, dataloader_size):\n    softmax = nn.Softmax(dim=1)\n    model = model.train()\n    running_l2_loss = AverageMeter()\n    running_total_loss = AverageMeter()\n    running_cross_entropy_loss = AverageMeter()\n\n    running_accuracy = AverageMeter()\n    labelled_itr = iter(labelled_dataloader)\n    pseudo_itr = iter(pseudo_labelled_dataloader)\n    for i in range(dataloader_size):\n        try:\n            sample = labelled_itr.next()\n            labelled_inputs, labelled_labels = sample['images'].to(device), sample['labels'].to(device)\n        except StopIteration:\n            labelled_itr = iter(labelled_dataloader)\n            sample = labelled_itr.next()\n            labelled_inputs, labelled_labels = sample['images'].to(device), sample['labels'].to(device)\n        try:\n            sample = pseudo_itr.next()\n            pseudo_inputs, pseudo_labels = sample['images'].to(device), sample['labels'].to(device)\n        except StopIteration:\n            print('Something wrong')\n            pseudo_itr = iter(pseudo_labelled_dataloader)\n            sample = pseudo_itr.next()\n            pseudo_inputs, pseudo_labels = sample['images'].to(device), sample['labels'].to(device)\n\n        labelled_outputs = model(labelled_inputs)\n        pseudo_outputs = softmax(model(pseudo_inputs))\n        _, labelled_predictions = torch.max(labelled_outputs, 1)\n        _, pseudo_predictions = torch.max(pseudo_outputs, 1)\n        _, pseudo_one_hot_labels = torch.max(pseudo_labels, 1)\n        loss1 = criterion1(labelled_outputs, labelled_labels)\n        loss2 = criterion2(pseudo_outputs.float(), pseudo_labels.float())\n        loss = loss1 + lambda_u * loss2\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_cross_entropy_loss.update(loss1.item(), labelled_inputs.size(0))\n        running_l2_loss.update(loss2.item(), labelled_inputs.size(0))\n        running_total_loss.update(loss.item(), 2 * labelled_inputs.size(0))\n        running_accuracy.update(accuracy(y_true=labelled_labels, y_pred=labelled_predictions), labelled_inputs.size(0))\n        running_accuracy.update(accuracy(y_true=pseudo_one_hot_labels, y_pred=pseudo_predictions),labelled_inputs.size(0))\n    print('Training Cross Entorpy Loss {}'.format(running_cross_entropy_loss.avg))\n    print('Training L2 Loss {}'.format(running_l2_loss.avg))\n    print('Training accuracy {}'.format(running_accuracy.avg))\n    print('T')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_start = time.time()\ncounter = 0\nbest_loss = 100000\nbest_loss_acc = 0.0\nepochs = 120\nbest_model = copy.deepcopy(model_res.state_dict())\nfor epoch in range(epochs):\n    print(\"\\n\")\n    print('Training Epoch {} __________'.format(epoch + 1))\n    train(model=model_res, labelled_dataloader=train_dataloader, pseudo_labelled_dataloader=sampled_pseudo_dataloader,\n          criterion1=criterion1, criterion2=criterion2, lambda_u=5, device=device, optimizer=optimizer, epoch=epoch,\n          dataloader_size=len(sampled_pseudo_dataloader))\n    \n    test_epoch_loss, test_epoch_acc = evaluate(model=model_res, dataloader=test_dataloader, epoch=epoch,\n                                               criterion=criterion1, dataloader_size=len(test_dataloader), device=device)\n    lr_scheduler.step()\n    if test_epoch_loss < best_loss:\n        counter = 0\n        best_loss = copy.deepcopy(test_epoch_loss)\n        best_loss_acc = copy.deepcopy(test_epoch_acc)\n        print('Best loss till now {:.4f}'.format(best_loss))\n        print('valid accuracy for this loss {:.4f}'.format(best_loss_acc))\n        print(\"saving the model\")\n        best_model = copy.deepcopy(model_res.state_dict())\n        \n    else:\n        counter = counter + 1\n        print(\"value of counter right now: \", counter)\n        if counter >= 7:\n            break\n        else:\n            pass\n\n# print(best_loss_acc)\n# Restoring best model\nmodel_res.load_state_dict(best_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_loss, best_loss_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model_res.state_dict(), 'res_model_no_aug_cifar100_10k_20k_lambda_5_sharp_03.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_mean_confidence(predictions, labels):\n    \"\"\"Computes Average Probability, Calibration Error and Accuracy\"\"\"\n    probabilities = np.max(predictions, 1)\n    acc = (labels == np.argmax(predictions, 1)).sum()\n    avg_prob = np.average(probabilities)\n   \n    return avg_prob, avg_prob - acc / labels.shape[0], acc / labels.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_res = []\nlabels_res = []\n\nsoftmax = nn.Softmax(dim=1)\nwith torch.no_grad():\n    model_res = model_res.eval()\n    for sample in tqdm(test_dataloader):\n        images = sample['images'].to(device)\n        labels = sample['labels'].to(device)\n        output = softmax(model_res(images))\n        predictions_res.append(output.cpu().numpy())\n        labels_res.append(labels.cpu().numpy())\n        \nprint(len(labels_res), len(predictions_res))\nprint(predictions_res[0].shape, labels_res[0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_res_final = predictions_res[0]\nlabels_res_final = labels_res[0]\nfor i in range(1, len(predictions_res)):\n    predictions_res_final = np.concatenate((predictions_res_final, predictions_res[i]))\n    labels_res_final = np.concatenate((labels_res_final, labels_res[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_score, cal_err, acc = calculate_mean_confidence(predictions_res_final, labels_res_final)\nprint(conf_score, cal_err, acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}